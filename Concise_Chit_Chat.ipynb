{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Concise Chit Chat.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zixia/concise-chit-chat/blob/master/Concise_Chit_Chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zrsIf6r8rPhh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Concise Chit Chat\n",
        "\n",
        "GitHub Repository: <https://github.com/zixia/concise-chit-chat>\n",
        "\n",
        "## Code TODO:\n",
        "\n",
        "1. create a DataLoader class for dataset preprocess. (Use tf.data.Dataset inside?)\n",
        "1. Create a PyPI package for easy load cornell movie curpos dataset(?)\n",
        "1. Use PyPI module `embeddings` to load `GLOVES`, or use tfhub to load `GLOVES`?\n",
        "1. How to do a `clip_norm`(or set `clip_value`) in Keras with Eager mode but without `tf.contrib`?\n",
        "1. Better name for variables & functions\n",
        "1. Code clean\n",
        "1. Encapsulate all layers to Model Class: \n",
        "    1. ChitChatEncoder\n",
        "    1. ChitChatDecoder\n",
        "    1. ChitChatModel\n",
        "1. Re-style to follow the book\n",
        "1. ...?\n",
        "\n",
        "## Book Todo\n",
        "\n",
        "1. Outlines\n",
        "1. What's seq2seq\n",
        "1. What's word embedding\n",
        "1. \n",
        "1. Split code into snips\n",
        "1. Write for snips\n",
        "1. Content cleaning and optimizing\n",
        "1. ...?\n",
        "\n",
        "## Other\n",
        "\n",
        "1. `keras.callbacks.TensorBoard` instead of `tf.contrib.summary`?\n",
        "    - `model.fit(callbacks=[TensorBoard(...)])`\n",
        "1. download url? - http://old.pep.com.cn/gzsx/jszx_1/czsxtbjxzy/qrzptgjzxjc/dzkb/dscl/"
      ]
    },
    {
      "metadata": {
        "id": "2ZMAKiFTor0E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### config.py"
      ]
    },
    {
      "metadata": {
        "id": "hcbpcp-gouRm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''doc'''\n",
        "\n",
        "# GO for start of the sentence\n",
        "# DONE for end of the sentence\n",
        "GO = '\\b'\n",
        "DONE = '\\a'\n",
        "\n",
        "# max words per sentence\n",
        "MAX_LEN = 20\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9UJJF8CGo1W_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### data_loader.py"
      ]
    },
    {
      "metadata": {
        "id": "1GLhqALTo4Rn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "data loader\n",
        "'''\n",
        "import gzip\n",
        "import re\n",
        "from typing import (\n",
        "    # Any,\n",
        "    List,\n",
        "    Tuple,\n",
        ")\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# from .config import (\n",
        "#     GO,\n",
        "#     DONE,\n",
        "#     MAX_LEN,\n",
        "# )\n",
        "\n",
        "DATASET_URL = 'https://github.com/zixia/concise-chit-chat/releases/download/v0.0.1/dataset.txt.gz'\n",
        "DATASET_FILE_NAME = 'concise-chit-chat-dataset.txt.gz'\n",
        "\n",
        "\n",
        "class DataLoader():\n",
        "    '''data loader'''\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        print('DataLoader', 'downloading dataset from:', DATASET_URL)\n",
        "        dataset_file = tf.keras.utils.get_file(\n",
        "            DATASET_FILE_NAME,\n",
        "            origin=DATASET_URL,\n",
        "        )\n",
        "        print('DataLoader', 'loading dataset from:', dataset_file)\n",
        "\n",
        "        # dataset_file = './data/dataset.txt.gz'\n",
        "\n",
        "        # with open(path, encoding='iso-8859-1') as f:\n",
        "        with gzip.open(dataset_file, 'rt') as f:\n",
        "            self.raw_text = f.read().lower()\n",
        "\n",
        "        self.queries, self.responses \\\n",
        "            = self.__parse_raw_text(self.raw_text)\n",
        "        self.size = len(self.queries)\n",
        "\n",
        "    def get_batch(\n",
        "            self,\n",
        "            batch_size=32,\n",
        "    ) -> Tuple[List[List[str]], List[List[str]]]:\n",
        "        '''get batch'''\n",
        "        # print('corpus_list', self.corpus)\n",
        "        batch_indices = np.random.choice(\n",
        "            len(self.queries),\n",
        "            size=batch_size,\n",
        "        )\n",
        "        batch_queries = self.queries[batch_indices]\n",
        "        batch_responses = self.responses[batch_indices]\n",
        "\n",
        "        return batch_queries, batch_responses\n",
        "\n",
        "    def __parse_raw_text(\n",
        "            self,\n",
        "            raw_text: str\n",
        "    ) -> Tuple[List[List[str]], List[List[str]]]:\n",
        "        '''doc'''\n",
        "        query_list = []\n",
        "        response_list = []\n",
        "\n",
        "        for line in raw_text.strip('\\n').split('\\n'):\n",
        "            query, response = line.split('\\t')\n",
        "            query, response = self.preprocess(query), self.preprocess(response)\n",
        "            query_list.append('{} {} {}'.format(GO, query, DONE))\n",
        "            response_list.append('{} {} {}'.format(GO, response, DONE))\n",
        "\n",
        "        return np.array(query_list), np.array(response_list)\n",
        "\n",
        "    def preprocess(self, text: str) -> str:\n",
        "        '''doc'''\n",
        "        new_text = text\n",
        "\n",
        "        new_text = re.sub('[^a-zA-Z0-9 .,?!]', ' ', new_text)\n",
        "        new_text = re.sub(' +', ' ', new_text)\n",
        "        new_text = re.sub(\n",
        "            '([\\w]+)([,;.?!#&-\\'\\\"-]+)([\\w]+)?',\n",
        "            r'\\1 \\2 \\3',\n",
        "            new_text,\n",
        "        )\n",
        "        if len(new_text.split()) > MAX_LEN:\n",
        "            new_text = (' ').join(new_text.split()[:MAX_LEN])\n",
        "            match = re.search('[.?!]', new_text)\n",
        "            if match is not None:\n",
        "                idx = match.start()\n",
        "                new_text = new_text[:idx+1]\n",
        "\n",
        "        new_text = new_text.strip().lower()\n",
        "\n",
        "        return new_text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0nwqN1Hbo5tu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### vocabulary.py"
      ]
    },
    {
      "metadata": {
        "id": "Hzw4W_C_o9IS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''doc'''\n",
        "import re\n",
        "from typing import (\n",
        "    List,\n",
        ")\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# from .config import (\n",
        "#     DONE,\n",
        "#     GO,\n",
        "#     MAX_LEN,\n",
        "# )\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    '''voc'''\n",
        "    def __init__(self, text: str) -> None:\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "        self.tokenizer.fit_on_texts(\n",
        "            [GO, DONE] + re.split(\n",
        "                r'[\\s\\t\\n]',\n",
        "                text,\n",
        "            )\n",
        "        )\n",
        "        # additional 1 for the index 0\n",
        "        self.size = 1 + len(self.tokenizer.word_index.keys())\n",
        "\n",
        "    def texts_to_padded_sequences(\n",
        "            self,\n",
        "            text_list: List[List[str]]\n",
        "    ) -> tf.Tensor:\n",
        "        '''doc'''\n",
        "        sequence_list = self.tokenizer.texts_to_sequences(text_list)\n",
        "        padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            sequence_list,\n",
        "            maxlen=MAX_LEN,\n",
        "            padding='post',\n",
        "            truncating='post',\n",
        "        )\n",
        "\n",
        "        return padded_sequences\n",
        "\n",
        "    def padded_sequences_to_texts(self, sequence: List[int]) -> str:\n",
        "        return 'tbw'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7BXN6mFGo-P2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### model.py"
      ]
    },
    {
      "metadata": {
        "id": "WdAVNAb0pAJb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''doc'''\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from typing import (\n",
        "    List,\n",
        ")\n",
        "\n",
        "# from .vocabulary import Vocabulary\n",
        "# from .config import (\n",
        "#     DONE,\n",
        "#     GO,\n",
        "#     MAX_LENGTH,\n",
        "# )\n",
        "\n",
        "EMBEDDING_DIM = 300\n",
        "LATENT_UNIT_NUM = 500\n",
        "\n",
        "\n",
        "class ChitEncoder(tf.keras.Model):\n",
        "    '''encoder'''\n",
        "    def __init__(\n",
        "            self,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.lstm_encoder = tf.keras.layers.CuDNNLSTM(\n",
        "            units=LATENT_UNIT_NUM,\n",
        "            return_state=True,\n",
        "        )\n",
        "\n",
        "    def call(\n",
        "            self,\n",
        "            inputs: tf.Tensor,  # shape: [batch_size, max_len, embedding_dim]\n",
        "            training=None,\n",
        "            mask=None,\n",
        "    ) -> tf.Tensor:\n",
        "        _, *state = self.lstm_encoder(inputs)\n",
        "        return state    # shape: ([latent_unit_num], [latent_unit_num])\n",
        "\n",
        "\n",
        "class ChatDecoder(tf.keras.Model):\n",
        "    '''decoder'''\n",
        "    def __init__(\n",
        "            self,\n",
        "            voc_size: int,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.lstm_decoder = tf.keras.layers.CuDNNLSTM(\n",
        "            units=LATENT_UNIT_NUM,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "        )\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            units=voc_size,\n",
        "        )\n",
        "\n",
        "        self.time_distributed_dense = tf.keras.layers.TimeDistributed(\n",
        "            self.dense\n",
        "        )\n",
        "\n",
        "        self.initial_state = None\n",
        "\n",
        "    def set_state(self, state=None):\n",
        "        '''doc'''\n",
        "        # import pdb; pdb.set_trace()\n",
        "        self.initial_state = state\n",
        "\n",
        "    def call(\n",
        "            self,\n",
        "            inputs: tf.Tensor,  # shape: [batch_size, None, embedding_dim]\n",
        "            training=False,\n",
        "            mask=None,\n",
        "    ) -> tf.Tensor:\n",
        "        '''chat decoder call'''\n",
        "\n",
        "        # batch_size = tf.shape(inputs)[0]\n",
        "        # max_len = tf.shape(inputs)[0]\n",
        "\n",
        "        # outputs = tf.zeros(shape=(\n",
        "        #     batch_size,         # batch_size\n",
        "        #     max_len,            # max time step\n",
        "        #     LATENT_UNIT_NUM,    # dimention of hidden state\n",
        "        # ))\n",
        "\n",
        "        # import pdb; pdb.set_trace()\n",
        "        outputs, *states = self.lstm_decoder(inputs, initial_state=self.initial_state)\n",
        "        self.initial_state = states\n",
        "\n",
        "        outputs = self.time_distributed_dense(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class ChitChat(tf.keras.Model):\n",
        "    '''doc'''\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocabulary: Vocabulary,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.word_index = vocabulary.tokenizer.word_index\n",
        "        self.index_word = vocabulary.tokenizer.index_word\n",
        "        self.voc_size = vocabulary.size\n",
        "\n",
        "        # [batch_size, max_len] -> [batch_size, max_len, voc_size]\n",
        "        self.embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=self.voc_size,\n",
        "            output_dim=EMBEDDING_DIM,\n",
        "            mask_zero=True,\n",
        "        )\n",
        "\n",
        "        self.encoder = ChitEncoder()\n",
        "        # shape: [batch_size, state]\n",
        "\n",
        "        self.decoder = ChatDecoder(self.voc_size)\n",
        "        # shape: [batch_size, max_len, voc_size]\n",
        "\n",
        "    def call(\n",
        "            self,\n",
        "            inputs: List[List[int]],  # shape: [batch_size, max_len]\n",
        "            teacher_forcing_targets: List[List[int]]=None,  # shape: [batch_size, max_len]\n",
        "            training=None,\n",
        "            mask=None,\n",
        "    ) -> tf.Tensor:     # shape: [batch_size, max_len, embedding_dim]\n",
        "        '''call'''\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "        inputs_embedding = self.embedding(tf.convert_to_tensor(inputs))\n",
        "        state = self.encoder(inputs_embedding)\n",
        "\n",
        "        self.decoder.set_state(state)\n",
        "\n",
        "        if training:\n",
        "            teacher_forcing_targets = tf.convert_to_tensor(teacher_forcing_targets)\n",
        "            teacher_forcing_embeddings = self.embedding(teacher_forcing_targets)\n",
        "\n",
        "        # outputs[:, 0, :].assign([self.__go_embedding()] * batch_size)\n",
        "        batch_go_embedding = tf.ones([batch_size, 1, 1]) * [self.__go_embedding()]\n",
        "        batch_go_one_hot = tf.ones([batch_size, 1, 1]) * [tf.one_hot(self.word_index[GO], self.voc_size)]\n",
        "\n",
        "        outputs = batch_go_one_hot\n",
        "        output = self.decoder(batch_go_embedding)\n",
        "\n",
        "        for t in range(1, MAX_LEN):\n",
        "            outputs = tf.concat([outputs, output], 1)\n",
        "            if training:\n",
        "                target = teacher_forcing_embeddings[:, t, :]\n",
        "                decoder_input = tf.expand_dims(target, axis=1)\n",
        "            else:\n",
        "                decoder_input = self.__indice_to_embedding(tf.argmax(output))\n",
        "\n",
        "            output = self.decoder(decoder_input)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def predict(self, inputs: List[int], temperature=1.) -> List[int]:\n",
        "        '''doc'''\n",
        "\n",
        "        outputs = self([inputs])\n",
        "        outputs = tf.squeeze(outputs)\n",
        "\n",
        "        word_list = []\n",
        "        for t in range(1, MAX_LEN):\n",
        "            output = outputs[t]\n",
        "\n",
        "            indice = self.__logit_to_indice(output, temperature=temperature)\n",
        "\n",
        "            word = self.index_word[indice]\n",
        "\n",
        "            if indice == self.word_index[DONE]:\n",
        "                break\n",
        "\n",
        "            word_list.append(word)\n",
        "\n",
        "        return ' '.join(word_list)\n",
        "\n",
        "    def __go_embedding(self) -> tf.Tensor:\n",
        "        return self.embedding(\n",
        "            tf.convert_to_tensor(self.word_index[GO]))\n",
        "\n",
        "    def __logit_to_indice(\n",
        "            self,\n",
        "            inputs,\n",
        "            temperature=1.,\n",
        "    ) -> int:\n",
        "        '''\n",
        "        [vocabulary_size]\n",
        "        convert one hot encoding to indice with temperature\n",
        "        '''\n",
        "        inputs = tf.squeeze(inputs)\n",
        "        prob = tf.nn.softmax(inputs / temperature).numpy()\n",
        "        indice = np.random.choice(self.voc_size, p=prob)\n",
        "        return indice\n",
        "\n",
        "    def __indice_to_embedding(self, indice: int) -> tf.Tensor:\n",
        "        tensor = tf.convert_to_tensor([[indice]])\n",
        "        return self.embedding(tensor)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5tLzYNJHom5r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train"
      ]
    },
    {
      "metadata": {
        "id": "DIW88lig-Meg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tensor Board\n",
        "\n",
        "[Quick guide to run TensorBoard in Google Colab](https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/)\n",
        "\n",
        "`tensorboard` vs `tensorboard/` ?"
      ]
    },
    {
      "metadata": {
        "id": "wQkFd2iz8jhi",
        "colab_type": "code",
        "outputId": "844791b8-8b39-4392-c258-6fd19caeb50d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "LOG_DIR = '/content/data/tensorboard/'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "# Install\n",
        "! npm install -g localtunnel\n",
        "\n",
        "# Tunnel port 6006 (TensorBoard assumed running)\n",
        "get_ipython().system_raw('lt --port 6006 >> url.txt 2>&1 &')\n",
        "\n",
        "# Get url\n",
        "! cat url.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/client\n",
            "\u001b[K\u001b[?25h+ localtunnel@1.9.1\n",
            "added 54 packages from 31 contributors in 2.667s\n",
            "your url is: https://tame-walrus-3.localtunnel.me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "255QU1tPopoL",
        "colab_type": "code",
        "outputId": "3b67bfc1-6985-4e26-9ed7-3f0bd8079acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3193
        }
      },
      "cell_type": "code",
      "source": [
        "'''train'''\n",
        "import tensorflow as tf\n",
        "\n",
        "# from chit_chat import (\n",
        "#     ChitChat,\n",
        "#     DataLoader,\n",
        "#     Vocabulary,\n",
        "# )\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "data_loader = DataLoader()\n",
        "vocabulary = Vocabulary(data_loader.raw_text)\n",
        "chitchat = ChitChat(vocabulary=vocabulary)\n",
        "\n",
        "\n",
        "def loss(model, x, y) -> tf.Tensor:\n",
        "    '''doc'''\n",
        "    weights = tf.cast(\n",
        "        tf.not_equal(y, 0),\n",
        "        tf.float32,\n",
        "    )\n",
        "\n",
        "    prediction = model(\n",
        "        inputs=x,\n",
        "        teacher_forcing_targets=y,\n",
        "        training=True,\n",
        "    )\n",
        "\n",
        "    # implment the following contrib function in a loop ?\n",
        "    # https://stackoverflow.com/a/41135778/1123955\n",
        "    # https://stackoverflow.com/q/48025004/1123955\n",
        "    return tf.contrib.seq2seq.sequence_loss(\n",
        "        prediction,\n",
        "        tf.convert_to_tensor(y),\n",
        "        weights,\n",
        "    )\n",
        "\n",
        "\n",
        "def grad(model, inputs, targets):\n",
        "    '''doc'''\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss(model, inputs, targets)\n",
        "\n",
        "    return tape.gradient(loss_value, model.variables)\n",
        "\n",
        "\n",
        "def train() -> int:\n",
        "    '''doc'''\n",
        "    learning_rate = 1e-3\n",
        "    num_batches = 8000\n",
        "    batch_size = 128\n",
        "\n",
        "    print('Dataset size: {}, Vocabulary size: {}'.format(\n",
        "        data_loader.size,\n",
        "        vocabulary.size,\n",
        "    ))\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "    root = tf.train.Checkpoint(\n",
        "        optimizer=optimizer,\n",
        "        model=chitchat,\n",
        "        optimizer_step=tf.train.get_or_create_global_step(),\n",
        "    )\n",
        "\n",
        "    root.restore(tf.train.latest_checkpoint('./data/save'))\n",
        "    print('checkpoint restored.')\n",
        "\n",
        "    writer = tf.contrib.summary.create_file_writer('./data/tensorboard')\n",
        "    writer.set_as_default()\n",
        "\n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "    for batch_index in range(num_batches):\n",
        "        global_step.assign_add(1)\n",
        "\n",
        "        queries, responses = data_loader.get_batch(batch_size)\n",
        "\n",
        "        encoder_inputs = vocabulary.texts_to_padded_sequences(queries)\n",
        "        decoder_outputs = vocabulary.texts_to_padded_sequences(responses)\n",
        "\n",
        "        grads = grad(chitchat, encoder_inputs, decoder_outputs)\n",
        "\n",
        "        optimizer.apply_gradients(\n",
        "            grads_and_vars=zip(grads, chitchat.variables)\n",
        "        )\n",
        "\n",
        "        if batch_index % 10 == 0:\n",
        "            print(\"batch %d: loss %f\" % (batch_index, loss(\n",
        "                chitchat, encoder_inputs, decoder_outputs).numpy()))\n",
        "            root.save('./data/save/model.ckpt')\n",
        "            print('checkpoint saved.')\n",
        "\n",
        "        with tf.contrib.summary.record_summaries_every_n_global_steps(1):\n",
        "            # your model code goes here\n",
        "            tf.contrib.summary.scalar('loss', loss(\n",
        "                chitchat, encoder_inputs, decoder_outputs).numpy())\n",
        "            # print('summary had been written.')\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n",
        "def main() -> int:\n",
        "    '''doc'''\n",
        "    return train()\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataLoader downloading dataset from: https://github.com/zixia/concise-chit-chat/releases/download/v0.0.1/dataset.txt.gz\n",
            "Downloading data from https://github.com/zixia/concise-chit-chat/releases/download/v0.0.1/dataset.txt.gz\n",
            "2310144/2304624 [==============================] - 2s 1us/step\n",
            "DataLoader loading dataset from: /root/.keras/datasets/concise-chit-chat-dataset.txt.gz\n",
            "Dataset size: 158015, Vocabulary size: 5001\n",
            "checkpoint restored.\n",
            "batch 0: loss 8.378550\n",
            "checkpoint saved.\n",
            "batch 10: loss 5.363611\n",
            "checkpoint saved.\n",
            "batch 20: loss 5.347839\n",
            "checkpoint saved.\n",
            "batch 30: loss 5.355069\n",
            "checkpoint saved.\n",
            "batch 40: loss 5.263589\n",
            "checkpoint saved.\n",
            "batch 50: loss 5.107776\n",
            "checkpoint saved.\n",
            "batch 60: loss 5.089061\n",
            "checkpoint saved.\n",
            "batch 70: loss 4.997322\n",
            "checkpoint saved.\n",
            "batch 80: loss 4.978524\n",
            "checkpoint saved.\n",
            "batch 90: loss 4.913832\n",
            "checkpoint saved.\n",
            "batch 100: loss 4.860683\n",
            "checkpoint saved.\n",
            "batch 110: loss 4.823889\n",
            "checkpoint saved.\n",
            "batch 120: loss 4.892643\n",
            "checkpoint saved.\n",
            "batch 130: loss 4.658375\n",
            "checkpoint saved.\n",
            "batch 140: loss 4.795345\n",
            "checkpoint saved.\n",
            "batch 150: loss 4.598568\n",
            "checkpoint saved.\n",
            "batch 160: loss 4.728464\n",
            "checkpoint saved.\n",
            "batch 170: loss 4.666811\n",
            "checkpoint saved.\n",
            "batch 180: loss 4.597885\n",
            "checkpoint saved.\n",
            "batch 190: loss 4.595983\n",
            "checkpoint saved.\n",
            "batch 200: loss 4.565698\n",
            "checkpoint saved.\n",
            "batch 210: loss 4.488758\n",
            "checkpoint saved.\n",
            "batch 220: loss 4.618034\n",
            "checkpoint saved.\n",
            "batch 230: loss 4.423510\n",
            "checkpoint saved.\n",
            "batch 240: loss 4.498684\n",
            "checkpoint saved.\n",
            "batch 250: loss 4.484734\n",
            "checkpoint saved.\n",
            "batch 260: loss 4.350574\n",
            "checkpoint saved.\n",
            "batch 270: loss 4.410893\n",
            "checkpoint saved.\n",
            "batch 280: loss 4.302926\n",
            "checkpoint saved.\n",
            "batch 290: loss 4.379134\n",
            "checkpoint saved.\n",
            "batch 300: loss 4.427905\n",
            "checkpoint saved.\n",
            "batch 310: loss 4.361248\n",
            "checkpoint saved.\n",
            "batch 320: loss 4.323555\n",
            "checkpoint saved.\n",
            "batch 330: loss 4.307600\n",
            "checkpoint saved.\n",
            "batch 340: loss 4.380899\n",
            "checkpoint saved.\n",
            "batch 350: loss 4.399303\n",
            "checkpoint saved.\n",
            "batch 360: loss 4.326337\n",
            "checkpoint saved.\n",
            "batch 370: loss 4.202069\n",
            "checkpoint saved.\n",
            "batch 380: loss 4.299171\n",
            "checkpoint saved.\n",
            "batch 390: loss 4.236771\n",
            "checkpoint saved.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6d0ad0c2be10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-6d0ad0c2be10>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m'''doc'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-6d0ad0c2be10>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_padded_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchitchat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         optimizer.apply_gradients(\n",
            "\u001b[0;32m<ipython-input-6-6d0ad0c2be10>\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(model, inputs, targets)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         output_gradients=output_gradients)\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       output_gradients)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_SparseSoftmaxCrossEntropyWithLogitsGrad\u001b[0;34m(op, grad_0, _)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;34m\"derivative of sparse_softmax_cross_entropy_with_logits due to the fused \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m       \"implementation's interaction with tf.gradients()\")\n\u001b[0;32m--> 497\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_BroadcastMul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_softmax_grad_without_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_BroadcastMul\u001b[0;34m(vec, mat)\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;31m# Reshape vec to [D0, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m   \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1129\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   5063\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5064\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5065\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[2560,5001] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul] name: mul/"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cOcnXxC3q3BI",
        "colab_type": "code",
        "outputId": "f67405ca-e322-42c0-b3a1-3485b21a6002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "cell_type": "code",
      "source": [
        "#! rm -fvr data/tensorboard\n",
        "# ! pwd\n",
        "# ! rm -frv data/save\n",
        "# ! rm -fr /content/data/tensorboard\n",
        "# ! kill 2823\n",
        "# ! kill -9 2823\n",
        "# ! ps axf | grep lt\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "removed 'data/save/model.ckpt-8.data-00000-of-00001'\n",
            "removed 'data/save/checkpoint'\n",
            "removed 'data/save/model.ckpt-11.data-00000-of-00001'\n",
            "removed 'data/save/model.ckpt-8.index'\n",
            "removed 'data/save/model.ckpt-1.data-00000-of-00001'\n",
            "removed 'data/save/model.ckpt-6.index'\n",
            "removed 'data/save/model.ckpt-4.index'\n",
            "removed 'data/save/model.ckpt-4.data-00000-of-00001'\n",
            "removed 'data/save/model.ckpt-12.index'\n",
            "removed 'data/save/model.ckpt-9.data-00000-of-00001'\n",
            "removed 'data/save/model.ckpt-3.data-00000-of-00001'\n",
            "removed 'data/save/model.ckpt-9.index'\n",
            "removed 'data/save/model.ckpt-7.index'\n",
            "removed 'data/save/model.ckpt-10.data-00000-of-00001'\n",
            "removed 'data/save/model.ckpt-2.index'\n",
            "removed 'data/save/model.ckpt-3.index'\n",
            "removed 'data/save/model.ckpt-6.data-00000-of-00001'\n",
            "removed 'data/save/model.ckpt-11.index'\n",
            "removed 'data/save/model.ckpt-7.data-00000-of-00001'\n",
            "removed 'data/save/model.ckpt-2.data-00000-of-00001'\n",
            "removed 'data/save/model.ckpt-1.index'\n",
            "removed 'data/save/model.ckpt-5.index'\n",
            "removed 'data/save/model.ckpt-10.index'\n",
            "removed 'data/save/model.ckpt-12.data-00000-of-00001'\n",
            "removed 'data/save/model.ckpt-5.data-00000-of-00001'\n",
            "removed directory 'data/save'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h-rzov2L-ur8",
        "colab_type": "code",
        "outputId": "707f8b18-8d6e-4677-88cb-a9d624670c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "! cat url.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "your url is: https://bright-fox-51.localtunnel.me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Guw3WxSfx20O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### chat.py"
      ]
    },
    {
      "metadata": {
        "id": "Ew8n-iUEx4E8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''train'''\n",
        "# import tensorflow as tf\n",
        "\n",
        "# from chit_chat import (\n",
        "#     ChitChat,\n",
        "#     DataLoader,\n",
        "#     Vocabulary,\n",
        "#     DONE,\n",
        "#     GO,\n",
        "# )\n",
        "\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "\n",
        "def main() -> int:\n",
        "    '''chat main'''\n",
        "    data_loader = DataLoader()\n",
        "    vocabulary = Vocabulary(data_loader.raw_text)\n",
        "\n",
        "    print('Dataset size: {}, Vocabulary size: {}'.format(\n",
        "        data_loader.size,\n",
        "        vocabulary.size,\n",
        "    ))\n",
        "\n",
        "    chitchat = ChitChat(vocabulary)\n",
        "    checkpoint = tf.train.Checkpoint(model=chitchat)\n",
        "    checkpoint.restore(tf.train.latest_checkpoint('./data/save'))\n",
        "    print('checkpoint restored.')\n",
        "\n",
        "    return cli(chitchat, vocabulary=vocabulary, data_loader=data_loader)\n",
        "\n",
        "\n",
        "def cli(chitchat: ChitChat, data_loader: DataLoader, vocabulary: Vocabulary):\n",
        "    '''command line interface'''\n",
        "    index_word = vocabulary.tokenizer.index_word\n",
        "    word_index = vocabulary.tokenizer.word_index\n",
        "    query = ''\n",
        "    while True:\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            query = input('> ').lower()\n",
        "            # Check if it is quit case\n",
        "            if query == 'q' or query == 'quit':\n",
        "                break\n",
        "            # Normalize sentence\n",
        "            query = data_loader.preprocess(query)\n",
        "            query = '{} {} {}'.format(GO, query, DONE)\n",
        "            # Evaluate sentence\n",
        "            query_sequence = vocabulary.texts_to_padded_sequences([query])[0]\n",
        "\n",
        "            response_sequence = chitchat.predict(query_sequence, 1)\n",
        "\n",
        "            # Format and print response sentence\n",
        "            response_word_list = [\n",
        "                index_word[indice]\n",
        "                for indice in response_sequence\n",
        "                if indice != 0 and indice != word_index[DONE]\n",
        "            ]\n",
        "\n",
        "            print('Bot:', ' '.join(response_word_list))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wI34ZygY-Nf-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "outputId": "cc16ebc9-5981-462e-d886-66bc07d96ccc"
      },
      "cell_type": "code",
      "source": [
        "! cat /proc/cpuinfo"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2299.998\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms xsaveopt arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\n",
            "bogomips\t: 4599.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2299.998\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms xsaveopt arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\n",
            "bogomips\t: 4599.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}